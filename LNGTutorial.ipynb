{
 "metadata": {
  "name": "LNGTutorial"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "A tutorial on Local n-grams for Authorship Attribution"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "By Dr. Robert Layton, Internet Commerce Security Laboratory, University of Ballarat."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO:\n",
      "\n",
      "* Dataset to be zipped and stored somewhere online, update the link in section (0)\n",
      "* Graphs showing comparison output\n",
      "* RLP, d1, d2, WPI\n",
      "* Code-as-file with command line interfaces\n",
      "* Equations and formal definitions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this tutorial, I will show you how to use local n-grams (LNG) methods for authorship attribution.\n",
      "LNG based methods are highly effective for authorship attribution, and represent one of the significant differences in techniques between authorship studies and other document based machine learning, such as topic modelling."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By the way, this tool is called the ipython notebook.\n",
      "It's free, it's easy to use and allows for using python in-line.\n",
      "I won't introduce it here, but for more information, have a look [here](http://ipython.org/notebook.html).\n",
      "You can download this notebook, and run it from your own computer, allowing you to change things in-line.\n",
      "Alternatively, you can copy and paste the code segments into a new file."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can run this code by either downloading this workbook, using the python interactive interpreter, or by copying the lines into a file and running them with python.\n",
      "I'll be assuming knowledge of how to run python scripts, and therefore won't be covering beginner knowledge here."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you find any mistakes or corrections, please contact me at r.layton@icsl.com.au"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Installation Dependencies"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I'll be using the python programming language, using numpy, scipy and scikit-learn as the 'stack'.\n",
      "You can install scikit-learn by following [this page](http://scikit-learn.org/stable/install.html), which will in turn install numpy and scipy.\n",
      "If you can run the following code, you are ready to run this tutorial.\n",
      "I've used version scikit-learn version 0.14 to do this tutorial, but it *should* work with version 0.12 and above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn\n",
      "print(sklearn.__version__)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.15-git\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "0) Utility Functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first utility function we need is a function for counting the frequency of character n-grams in a string.\n",
      "A character n-gram is a sequence of *n* characters in a row, present in a string.\n",
      "For these examples, we assume `n=3`, with these n-grams also called *tri-grams*.\n",
      "For example:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = 3  # Length of n-gram\n",
      "suess = \"Sam! If you will let me be, I will try them. You will see.\"\n",
      "for i in range(5):  # Print the first five n-grams\n",
      "    print(\"n-gram #{}: {}\".format(i, suess[i:i+n]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "n-gram #0: Sam\n",
        "n-gram #1: am!\n",
        "n-gram #2: m! \n",
        "n-gram #3: ! I\n",
        "n-gram #4:  If\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We won't do any preprocessing in this tutorial, so capital letters, numbers and punctuation will all remain as they are in the original documents.\n",
      "\n",
      "Now, we will use the following function to extract and count all n-grams from a given string, which I will refer to as a document.\n",
      "The counts will be optionally normalised so that the figures are the frequencies, summing to 1.0."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# A defaultdict works the same as a normal dict, except\n",
      "# that if the key is not found, it is given a default value.\n",
      "from collections import defaultdict\n",
      "\n",
      "\n",
      "def count_ngrams(document, n, normalise=False):    \n",
      "    counts = defaultdict(float)  # Default to 0.0 if the key not found\n",
      "    # Iterate through all n-grams in the document\n",
      "    for i in range(len(document) - n + 1):\n",
      "        ngram = document[i:i+n]\n",
      "        # Update the count of this n-gram.\n",
      "        counts[ngram] = counts[ngram] + 1\n",
      "    if normalise:\n",
      "        # Normalise so that sums equal 1\n",
      "        normalise_factor = float(len(document) - n + 1)\n",
      "        for ngram in counts:\n",
      "            counts[ngram] /= normalise_factor\n",
      "    return counts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "counts = count_ngrams(suess, 3)\n",
      "# Print some examples\n",
      "for ngram in [\" I \", \"Sam\", \"ill\", \"abc\"]:\n",
      "    print(\"{0} occurs {1:.0f} time(s)\".format(ngram, counts[ngram]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " I  occurs 1 time(s)\n",
        "Sam occurs 1 time(s)\n",
        "ill occurs 3 time(s)\n",
        "abc occurs 0 time(s)\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frequencies = count_ngrams(suess, 3, normalise=True)\n",
      "# Print some examples\n",
      "for ngram in [\" I \", \"Sam\", \"ill\", \"abc\"]:\n",
      "    print(\"{0} has frequency {1:.3f}\".format(ngram, frequencies[ngram]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " I  has frequency 0.018\n",
        "Sam has frequency 0.018\n",
        "ill has frequency 0.054\n",
        "abc has frequency 0.000\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now need a corpus to do the classification on.\n",
      "The format I'll use for this tutorial is to use one list of strings for the \"documents\" and one array of integers as the \"classes\", or \"targets\".\n",
      "Each document has a corresponding class value, such that all documents with the same class were written by the same person.\n",
      "\n",
      "\n",
      "First, grab [this zip file](http://dataset.goes.here) containing the dataset and unzip it into a directory somewhere.\n",
      "Then update `default_folder` value below, changing the value from `None` to the path of the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "from os.path import expanduser\n",
      "import codecs\n",
      "import numpy as np\n",
      "\n",
      "default_folder = None  # Change this to the folder name containing the dataset\n",
      "\n",
      "def get_single_corpus(folder=None):\n",
      "    if folder is None:\n",
      "        folder = os.path.join(expanduser(\"~\"), \"Data\", \"books\")\n",
      "    documents = []\n",
      "    authors = []\n",
      "    training_mask = []\n",
      "    authornum = 0\n",
      "    i = 0\n",
      "    subfolders = [name for name in os.listdir(folder)\n",
      "                  if os.path.isdir(os.path.join(folder, name))]\n",
      "    for subfolder in subfolders:\n",
      "        sf = os.path.join(folder, subfolder)\n",
      "        #print \"Author %d is %s\" % (authornum, subfolder)\n",
      "        for document_name in os.listdir(sf):\n",
      "            i += 1\n",
      "            with codecs.open(os.path.join(sf, document_name), encoding='utf=8') as input_f:\n",
      "                documents.append(cleanFile(input_f.read()))\n",
      "                authors.append(authornum)\n",
      "                training_mask.append(True)\n",
      "        authornum += 1\n",
      "    return documents, np.array(authors, dtype='int')\n",
      "\n",
      "def cleanFile(document):\n",
      "    # Removes the stuff pg put in the ebooks\n",
      "    lines = document.split(\"\\n\")\n",
      "    start = 0\n",
      "    end = len(lines)\n",
      "    for i in range(len(lines)):\n",
      "        line = lines[i]\n",
      "        if line.startswith(\"*** START OF THIS PROJECT GUTENBERG\"):\n",
      "            start = i + 1\n",
      "        elif line.startswith(\"*** END OF THIS PROJECT GUTENBERG\"):\n",
      "            end = i - 1\n",
      "    return \"\\n\".join(lines[start:end])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "documents, classes = get_single_corpus()\n",
      "documents = np.array(documents, dtype=object)  # For fancy indexing using numpy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Example document, first 20 lines\n",
      "print(documents[0].split(\"\\n\")[:20])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'\\r', u'\\r', u'\\r', u'\\r', u'Produced by Dave Morgan, Martin Pettit and the Online Distributed\\r', u'Proofreading Team\\r', u'\\r', u'\\r', u'\\r', u'\\r', u'\\r', u\"[Transcribers note: Authors 'R.N and J.N.' are Robert Naylor and John Naylor.]\\r\", u'\\r', u'\\r', u'[Illustration: Mr. Robert Naylor FROM A PHOTOGRAPH TAKEN DURING HIS\\r', u'CANDIDATURE FOR THE REPRESENTATION OF THE CARNAVON BOROUGHS 1906]\\r', u'\\r', u'\\r', u'\\r', u'\\r']\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "# Print stats about the authors\n",
      "print(\"\\n\".join(\"Author {} appears {} times\".format(*r) for r in enumerate(np.bincount(classes))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Author 0 appears 1 times\n",
        "Author 1 appears 10 times\n",
        "Author 2 appears 44 times\n",
        "Author 3 appears 11 times\n",
        "Author 4 appears 29 times\n",
        "Author 5 appears 51 times\n",
        "Author 6 appears 22 times\n",
        "Author 7 appears 10 times\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That's all the setup that is needed for this tutorial."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1) Source Code Author Profiles (SCAP)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "SCAP was not the first LNG method to be created, but it is the easiest to explain.\n",
      "In SCAP, documents are *profiled* by taking the *L* most frequent n-grams that occur in the document.\n",
      "An author is profiled in a similar way, except the *L* most frequent n-grams from all known documents from that author are used.\n",
      "Two profiles are compared by finding the number of n-grams each profile has in common, which can be normalised by dividing by L.\n",
      "The more n-grams they have in common, the more similar the profiles are.\n",
      "When trying to determine which author wrote a particular document, the most similar profile.\n",
      "To make this calculation consistent with other methods that will be introduced later, we use a distance instead of similarity.\n",
      "The distance is just `1 - s`, where s is the similarity.\n",
      "In this format, the lower the distance, the more likely the author, and the author with the lowest distance to a document is the most likely author of that document.\n",
      "\n",
      "For two profiles $P_1$ and $P_2$ (which can be either author or document profiles) the distance is given below.\n",
      "An n-gram $x$ is in $P_i$ if it is in the profile, and the intersection of two profiles is the set of n-grams that are in both.\n",
      "\n",
      "*As a similarity:*\n",
      "$$s_{SCAP}(P_1, P_2) = \\frac{1}{L}|P_1 \\cap P_2|$$\n",
      "\n",
      "*As a distance:*\n",
      "$$d_{SCAP}(P_1, P_2) = min(0, 1 - s_{SCAP})$$ \n",
      "\n",
      "\n",
      "The class below implements SCAP using the base classes from scikit-learn (imported as sklearn).\n",
      "This will be important later, but for now, the important thing to acknoweldge is the `fit` and `predict` functions.\n",
      "The `fit` function takes a set of training data and creates profiles for each author in that data.\n",
      "The `predict` function predicts which of those authors is most likely to have written each document.\n",
      "The class also has utility functions for creating profiles and comparing profiles.\n",
      "\n",
      "The reference for this paper is below:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "@inproceedings{frantzeskou2006scap,\n",
      "\ttitle = {Effective identification of source code authors using byte-level information},\n",
      "\tbooktitle = {Proceedings of the 28th international conference on Software engineering},\n",
      "\tauthor = {Frantzeskou, Georgia and Stamatatos, Efstathios and Gritzalis, Stefanos and Katsikas, Sokratis},\n",
      "\tyear = {2006},\n",
      "\tpages = {893\u2013896},\n",
      "}"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from operator import itemgetter\n",
      "from sklearn.base import BaseEstimator, ClassifierMixin  # For using scikit-learn's test framework. Ignore for now\n",
      "\n",
      "class SCAP(BaseEstimator, ClassifierMixin):\n",
      "    def __init__(self, n, L):\n",
      "        self.n = n\n",
      "        self.L = L\n",
      "        self.author_profiles = None  # Will be trained by fit()\n",
      "\n",
      "    def create_profile(self, documents):\n",
      "        # Creates a profile of a document or list of documents.\n",
      "        if isinstance(documents, basestring):\n",
      "            # documents can be either a list of documents, or a single document.\n",
      "            # if it's a single document, convert to a list\n",
      "            documents = [documents,]\n",
      "        # profile each document independently\n",
      "        profiles = (count_ngrams(document, self.n, normalise=False)\n",
      "                    for document in documents)\n",
      "        # Merge the profiles\n",
      "        main_profile = defaultdict(float)\n",
      "        for profile in profiles:\n",
      "            for ngram in profile:\n",
      "                main_profile[ngram] += profile[ngram]\n",
      "        # Normalise the profile\n",
      "        num_ngrams = sum(main_profile.values())\n",
      "        for ngram in main_profile:\n",
      "            main_profile[ngram] /= num_ngrams\n",
      "        # Return the profile with only the top L n-grams\n",
      "        return self.top_L(main_profile)\n",
      "\n",
      "    def top_L(self, profile):\n",
      "        # Returns the profile with only the top L most frequent n-grams\n",
      "        threshold = sorted(profile.values(), reverse=True)[self.L]\n",
      "        return {ngram: profile[ngram]\n",
      "                for ngram in profile\n",
      "                if profile[ngram] >= threshold}\n",
      "\n",
      "    def compare_profiles(self, profile1, profile2):\n",
      "        # Number of n-grams in both profiles, divided by L\n",
      "        similarity =  len(set(profile1.keys()) & set(profile2.keys())) / float(self.L)\n",
      "        # Slight edge case here, similarity could be higher than 1.\n",
      "        # Just make it equal to 1 if it is over.\n",
      "        similarity = min(similarity, 1.0)\n",
      "        distance = 1 - similarity\n",
      "        return distance\n",
      "\n",
      "    def fit(self, documents, classes):\n",
      "        # Fits the current model to the training data provided\n",
      "        # Separate documents into the sets written by each author\n",
      "        author_documents = ((author, [documents[i] \n",
      "                                      for i in range(len(documents))\n",
      "                                      if classes[i] == author])\n",
      "                            for author in set(classes))\n",
      "        # Profile each of the authors independently\n",
      "        self.author_profiles = {author:self.create_profile(cur_docs)\n",
      "                                for author, cur_docs in author_documents}\n",
      "    \n",
      "    def predict(self, documents):\n",
      "        # Predict which of the authors wrote each of the documents\n",
      "        predictions = np.array([self.predict_single(document) for document in documents])\n",
      "        return predictions\n",
      "\n",
      "    def predict_single(self, document):\n",
      "        # Predicts the author of a single document\n",
      "        # Profile of current document\n",
      "        profile = self.create_profile(document)\n",
      "        # Distance from document to each author\n",
      "        distances = [(author, self.compare_profiles(profile, self.author_profiles[author]))\n",
      "                      for author in self.author_profiles]\n",
      "        # Get the nearest pair, and the author from that pair\n",
      "        prediction = sorted(distances, key=itemgetter(1))[0][0]\n",
      "        return prediction"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "OK, let's see it in action. I'll point out here that this method of testing should not be used in the real world. The fundamental mistake I'll be making is to use training data to test the method, which should never be done in practice. Anyway, let's do it now:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = SCAP(n=4, L=2000)\n",
      "# Training the model\n",
      "model.fit(documents, classes)\n",
      "# Predict the author of each of the documents\n",
      "y_pred = model.predict(documents)  # Notice how I am predicting the data I used for training? Don't do that.\n",
      "# Compute the accuracy of these predictions.\n",
      "print(\"Accuracy is {:.1f}%\".format(100. * np.mean(classes == y_pred)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy is 80.9%\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not a bad result, but as I said, don't use training data for testing.\n",
      "Here is a function that uses scikit-learn to perform the testing, which automatically splits the data into training and testing data before computing the accuracy, through a process called cross fold validation.\n",
      "More information on this is available in the [scikit-learn documentation](http://scikit-learn.org/stable/modules/cross_validation.html), but I won't go into the details here.\n",
      "In a nutshell, what cross validation does is split the data into two sets, training and testing.\n",
      "It does this multiple times with different splits of the data (depending on the exact method used)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First, setup the cross fold validation object\n",
      "from sklearn import cross_validation\n",
      "cv = cross_validation.KFold(len(documents), n_folds=5, random_state=0)\n",
      "# Now, compute the score using it\n",
      "scores = cross_validation.cross_val_score(model, documents, classes, cv=cv)\n",
      "print(\"Accuracy is {:.1f}%\".format(100. * np.mean(scores)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy is 48.3%\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A fair bit lower, but not too bad (the chance rate is around 28%). This is a better representation of what we can expect the algorithm to do if applied to real data with these parameters."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2) Common n-grams (CNG)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "CNG was the first LNG method proposed in the literature.\n",
      "The method is almost entirely the same as SCAP (should that be the other way around?), except for the method used to compute the distance.\n",
      "This distance computation uses the frequencies of the n-grams, where SCAP simply used the fact they were in the top *L* n-grams.\n",
      "\n",
      "$$ d_{CNG}(P_1, P_2) = \\displaystyle\\sum_{x \\in X_{P_1} \\cup X_{P_2}} \\Biggl(\\frac{2\\cdot(P_1(x) - P_2(x))}{P_1(x) + P_2(x)}\\Biggr)^2 $$\n",
      "\n",
      "Note that $P_i(x)$ is the frequency of n-gram $x$ in profile $P_i$, which is zero if the n-gram is not in the profile.\n",
      "(This is important, as it is zero if it is not in the top L, even if it does have a frequency in the original document.)\n",
      "\n",
      "To simplify the coding, we simply derive our new class from the SCAP class defined above. The reference for this method is below."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "@inproceedings{kevsel2003cng,\n",
      "\ttitle = {N-gram-based author profiles for authorship attribution},\n",
      "\tvolume = {3},\n",
      "\tbooktitle = {Proceedings of the Conference Pacific Association for Computational Linguistics, {PACLING}},\n",
      "\tauthor = {Ke{\\textbackslash}vselj, Vlado and Peng, Fuchun and Cercone, Nick and Thomas, Calvin},\n",
      "\tyear = {2003},\n",
      "\tpages = {255\u2013264}\n",
      "}"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class CNG(SCAP):\n",
      "    def compare_profiles(self, profile1, profile2):\n",
      "        ngrams = set(profile1.keys() + profile2.keys())\n",
      "        d1 = np.array([profile1.get(ng, 0.) for ng in ngrams])\n",
      "        d2 = np.array([profile2.get(ng, 0.) for ng in ngrams])\n",
      "        return np.mean(4 * np.square((d1 - d2) / (d1 + d2 + 1e-16)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Thanks to the fact we inherited from the above `SCAP` class, testing this method uses almost exactly the same code.\n",
      "The only difference in the code below has been highlighted."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Setup a new model, this is the only difference\n",
      "model = CNG(n=4, L=2000)\n",
      "# First, setup the cross fold validation object\n",
      "from sklearn import cross_validation\n",
      "cv = cross_validation.KFold(len(documents), n_folds=5, random_state=0)\n",
      "# Now, compute the score using it\n",
      "scores = cross_validation.cross_val_score(model, documents, classes, cv=cv)\n",
      "print(\"Accuracy is {:.1f}%\".format(100. * np.mean(scores)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy is 49.9%\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Slightly better than before. One problem here is that the values of the parameters (`n` and `L` in this case) make a huge difference in the accuracy of the algorithm.\n",
      "To find good algorithms, we can perform a grid search of different parameters.\n",
      "\n",
      "Thanks to the format of the classes, we can use scikit-learn's grid search parameter for this purpose.\n",
      "I've greatly restricted the parameter spaces, as this can take a long time to compute."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This code snippet is a shortened version of the one in the scikit-learn docs, available here:\n",
      "# http://scikit-learn.org/stable/auto_examples/grid_search_digits.html#example-grid-search-digits-py\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.metrics import classification_report\n",
      "parameters = [{'n': [3, 4], 'L':[500, 1000]},]  # These are the parameters to search through.\n",
      "\n",
      "# The n and L values here don't matter, they will be updated for training.\n",
      "clf = GridSearchCV(CNG(n=1, L=1), parameters, cv=5, scoring='accuracy')\n",
      "clf.fit(documents, classes)\n",
      "print(\"The best model found has n={} and L={}\".format(clf.best_estimator_.n, clf.best_estimator_.L))\n",
      "print(\"The scores for the different parameter sets were:\")\n",
      "for params, mean_score, scores in clf.grid_scores_:\n",
      "    print(\"{}: {:.3f} (+/-{:.3f})\".format(params, mean_score, scores.std() / 2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The best model found has n=4 and L=500\n",
        "The scores for the different parameter sets were:\n",
        "{'L': 500, 'n': 3}: 0.747 (+/-0.023)\n",
        "{'L': 500, 'n': 4}: 0.803 (+/-0.038)\n",
        "{'L': 1000, 'n': 3}: 0.758 (+/-0.026)\n",
        "{'L': 1000, 'n': 4}: 0.803 (+/-0.038)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:397: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=5.\n",
        "  % (min_labels, self.n_folds)), Warning)\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From this, we can increase the parameter options and do larger searches.\n",
      "I recommend moving to a more distributed system, and having a look at the `n_jobs` parameter, which can run the search across multiple processes (see the [docs](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html) for more details).\n",
      "\n",
      "The returned model is a valid scikit-learn estimator, so you can call the below code, which will use this model.\n",
      "Additionally, you can retrieve the actual best estimator instance by calling `clf.best_estimator_`.\n",
      "One final word of caution here about the next snippet: I have used the documents list, which was used in training the model, for the evaluation.\n",
      "You shouldn't do that, instead, you should split the data up into a training and evaluation set, where the evaluation set is used *only at the end* and never in any training or cross validation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_true, y_pred = classes, clf.predict(documents)\n",
      "print(classification_report(y_true, y_pred))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       1.00      1.00      1.00         1\n",
        "          1       0.75      0.90      0.82        10\n",
        "          2       1.00      0.61      0.76        44\n",
        "          3       0.85      1.00      0.92        11\n",
        "          4       0.63      0.90      0.74        29\n",
        "          5       0.92      0.92      0.92        51\n",
        "          6       0.86      0.86      0.86        22\n",
        "          7       0.91      1.00      0.95        10\n",
        "\n",
        "avg / total       0.87      0.84      0.84       178\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3) Recentred Local Profiles (RLP)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "RLP was created by myself, and has a number of differences to the first two algorithms presented.\n",
      "At its core though, it is based on the principles as CNG (and therefore SCAP).\n",
      "\n",
      "In the original derivation of CNG, the work of Bennet (1976), which used a similar model, with a language default value.\n",
      "This language default is effectively a profile of all documents in that language.\n",
      "Then, rather than compare two profiles, we first normalise the profiles against what is the normal value for that n-gram.\n",
      "Intuitively, while the `the` n-gram is very common for most authors, it is expected to be, and therefore the presence of this n-gram is hardly surprising.\n",
      "By comparing to the language default, we see that this n-gram is expected and therefore are less surprised.\n",
      "A profile is then computed by first computing the CNG-esque profile of the author, then subtracting (recentering) the values from the corpus default profile.\n",
      "These profiles are then compared in RLP.\n",
      "\n",
      "RLP does a number of other things differently as well.\n",
      "Rather then setting n-gram frequencies to zero if they are not present in the profile (as in CNG), the actual frequency value is used for comparisons, even if that n-gram is not present.\n",
      "Profiles are compared using the following equation, where $||P_i||_2$ is the Euclidean distance of the profile (i.e. the sum of the squares of each frequency value, and $P_i \\cdot P_j$ is the dot product of the n-gram values in each profile, for all n-grams in the profiles' union.\n",
      "Another major change is that the `top L` n-grams are ranked by absolute, rather than actual highest.\n",
      "This means an n-gram with a value of -0.15 occurs higher than an n-gram with value 0.14.\n",
      "\n",
      "                                                                                                            \n",
      "Two profiles are then compared using the following equation, which is also known as the cosine distance.                                                                                                             \n",
      "\n",
      "$$ d_{RLP}(P_1, P_2) = 1 - \\frac{P_1 \\cdot P_2}{||P_1||_2 ||P_2||_2}  $$\n",
      "\n",
      "References:\n",
      "    "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "@book{bennett1976scientific,\n",
      "  title={Scientific and engineering problem-solving with the computer},\n",
      "  author={Bennett, William Ralph},\n",
      "  year={1976},\n",
      "  publisher={Prentice Hall PTR}\n",
      "}\n",
      "\n",
      "@article{layton2012recentred,\n",
      "  title={Recentred local profiles for authorship attribution.},\n",
      "  author={Layton, Robert and Watters, Paul Andrew and Dazeley, Richard},\n",
      "  journal={Natural Language Engineering},\n",
      "  volume={18},\n",
      "  number={3},\n",
      "  pages={293--312},\n",
      "  year={2012},\n",
      "  publisher={Cambridge Univ Press}\n",
      "}"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.spatial import distance\n",
      "\n",
      "\n",
      "class RLP(SCAP):\n",
      "    \n",
      "    def __init__(self, n, L):\n",
      "        super(RLP, self).__init__(n, L)\n",
      "        self.topLonly = False  # Don't limit profiles on creation\n",
      "        self.language_profile = None\n",
      "    \n",
      "    def compare_profiles(self, profile1, profile2):\n",
      "        # All n-grams in the top L of either profile.\n",
      "        ngrams = set(self.top_L(profile1).keys() + self.top_L(profile1).keys())\n",
      "        # Profile vector for profile 1\n",
      "        d1 = np.array([profile1.get(ng, 0.) for ng in ngrams])\n",
      "        # Profile vector for profile 2\n",
      "        d2 = np.array([profile2.get(ng, 0.) for ng in ngrams])\n",
      "        return distance.cosine(d1, d2)\n",
      "    \n",
      "    def top_L(self, profile):\n",
      "        threshold = sorted(map(abs, profile.values()), reverse=True)[self.L]\n",
      "        copy = defaultdict(float)\n",
      "        for key in profile:\n",
      "            if abs(profile[key]) >= threshold:\n",
      "                copy[key] = profile[key]\n",
      "        return copy\n",
      "    \n",
      "    def fit(self, documents, classes):\n",
      "        self.language_profile = self.create_profile(documents)\n",
      "        super(RLP, self).fit(documents, classes)\n",
      "        \n",
      "    def create_profile(self, documents):\n",
      "        # Creates a profile of a document or list of documents.\n",
      "        if isinstance(documents, basestring):\n",
      "            # documents can be either a list of documents, or a single document.\n",
      "            # if it's a single document, convert to a list\n",
      "            documents = [documents,]\n",
      "        # profile each document independently\n",
      "        profiles = (count_ngrams(document, self.n, normalise=False)\n",
      "                    for document in documents)\n",
      "        # Merge the profiles\n",
      "        main_profile = defaultdict(float)\n",
      "        for profile in profiles:\n",
      "            for ngram in profile:\n",
      "                main_profile[ngram] += profile[ngram]\n",
      "        # Normalise the profile\n",
      "        num_ngrams = float(sum(main_profile.values()))\n",
      "        for ngram in main_profile:\n",
      "            main_profile[ngram] /= num_ngrams\n",
      "        if self.language_profile is not None:\n",
      "            # Recentre profile.\n",
      "            for key in main_profile:\n",
      "                main_profile[key] = main_profile.get(key, 0) - self.language_profile.get(key, 0)\n",
      "        # Note that the profile is returned in full, as exact frequencies are used\n",
      "        # in comparing profiles (rather than chopped off)\n",
      "        return main_profile\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 111
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we test with out overfitting code from earlier."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = RLP(n=4, L=2000)\n",
      "# Training the model\n",
      "model.fit(documents, classes)\n",
      "# Predict the author of each of the documents\n",
      "y_pred = model.predict(documents)  # Notice how I am predicting the data I used for training? Don't do that.\n",
      "# Compute the accuracy of these predictions.\n",
      "print(\"Accuracy is {:.1f}%\".format(100. * np.mean(classes == y_pred)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy is 71.3%\n"
       ]
      }
     ],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Have a look at some of the values in the language profile\n",
      "for ngram in ['the ', \"'ell\", \" and\", \"est \", \"ever\"]:\n",
      "    print(\"{} = {:.6f}\".format(repr(ngram), model.language_profile[ngram]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "'the ' = 0.008043\n",
        "\"'ell\" = 0.000011\n",
        "' and' = 0.005161\n",
        "'est ' = 0.000346\n",
        "'ever' = 0.000773\n"
       ]
      }
     ],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Have a look at one of the author profiles\n",
      "ap = model.author_profiles[1]\n",
      "print(\"Length of profile is {}\".format(len(ap))) # Notice how this is much higher than L (which is currently 2000)\n",
      "for ngram in ['the ', \"'ell\", \" and\", \"est \", \"ever\"]:\n",
      "    print(\"{} = {:.6f}\".format(repr(ngram), ap[ngram]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Length of profile is 72399\n",
        "'the ' = 0.000767\n",
        "\"'ell\" = 0.000000\n",
        "' and' = 0.001231\n",
        "'est ' = -0.000089\n",
        "'ever' = 0.000136\n"
       ]
      }
     ],
     "prompt_number": 110
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice how the n-gram `est ` has a negative frequency.\n",
      "This means the frequency for this author was 0.000089 lower than the frequency for the language default.\n",
      "RLP uses this information, where it may be lost in other LNG methods.\n",
      "The cost, as you may be discovered if you are running the code as we go, is that RLP is *significantly* slower than CNG or SCAP.\n",
      "Experiments I've conducted show that RLP is typically higher than CNG (which is in turn typically higher than SCAP) for most corpora.\n",
      "\n",
      "Next, we do a more appropriate test using cross validation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Setup a new model\n",
      "model = RLP(n=4, L=2000)\n",
      "# First, setup the cross fold validation object\n",
      "from sklearn import cross_validation\n",
      "cv = cross_validation.KFold(len(documents), n_folds=5, random_state=0)\n",
      "# Now, compute the score using it\n",
      "scores = cross_validation.cross_val_score(model, documents, classes, cv=cv)\n",
      "print(\"Accuracy is {:.1f}%\".format(100. * np.mean(scores)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy is 46.6%\n"
       ]
      }
     ],
     "prompt_number": 115
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That's lower than before, but with different models comes different parameters for improving performance.\n",
      "A grid search through some parameter values will give a better score.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.metrics import classification_report\n",
      "parameters = [{'n': [3, 4], 'L':[500, 1000]},]  # These are the parameters to search through.\n",
      "\n",
      "# The n and L values here don't matter, they will be updated for training.\n",
      "clf = GridSearchCV(RLP(n=1, L=1), parameters, cv=5, scoring='accuracy')\n",
      "clf.fit(documents, classes)\n",
      "print(\"The best model found has n={} and L={}\".format(clf.best_estimator_.n, clf.best_estimator_.L))\n",
      "print(\"The scores for the different parameter sets were:\")\n",
      "for params, mean_score, scores in clf.grid_scores_:\n",
      "    print(\"{}: {:.3f} (+/-{:.3f})\".format(params, mean_score, scores.std() / 2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}